{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install/update python, then install the following python libs:\n",
    "\n",
    "- pip install opencv-contrib-python (computer vision lib used for image editing (contrib installs all contributions))\n",
    "- pip install tensorflow            (used for working with deep-learning networks)\n",
    "- pip install numpy                 (used for working with matrices)\n",
    "- pip install matplotlib            (this is used to visualize digits and points like MATLAB (optional))\n",
    "- Also install the Black Formatter extension for python formatting on save (equivalent of prettier for js)\n",
    "- To run this file, cd to Handwritting_recognizer then run: **python src/main.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will load the dataset directly from tf module instead of downloading csv files and pre-processing them\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now during training we have labelled data meaning we know what digit each picture corresponds to\n",
    "- We split the labelled data into two parts: 1. Training data (80% of labelled data), 2. Testing data (20%)\n",
    "- But in this case we use the load data function from mnist dataset which is already 80:20 splitted\n",
    "- x-data refers to the actual image as a flattened matric of pixels (it's just the pixel data)\n",
    "- y-data refers to the labelled digit i.e. what digit the image actually is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns two tuples (destructuring)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we will normalize it meaning scale it down between 0 and 1, e.g. grayscale intensity value is between 0 to 255\n",
    "- On normalizing it we get between 0 and 1\n",
    "- Keep in mind we only wanna normalize the pixel values and not the labelling digits to make it easier for neural network\n",
    "- to do the calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "plt.imshow(x_train[10], cmap=\"gray\")\n",
    "plt.show()\n",
    "# print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also take a look at from the README.md file:\n",
    "- Activation functions\n",
    "- Optimizers\n",
    "- Loss functions\n",
    "- Layer types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>Building the model by adding layers, compiling, fitting (training) and saving the model</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the pre-processing part is over, now we will build the model and train it\n",
    "model = tf.keras.models.Sequential()  # Just a basic sequential neural network\n",
    "\n",
    "# Now we add each layer to our model using model.add(layerType) function\n",
    "# A flatten layer takes a p x q matrix and flattens it out into a list/array of p x q elements | Input layer\n",
    "\n",
    "# here the mnist dataset is in 28x28 pixels so we use 28x28 matrix\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))  # first layer\n",
    "\n",
    "# Dense layer definition in README.md | We will be adding two dense layers\n",
    "model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Now we will be adding the Output layer with ten neurons cause 10 digits = 10 possible outputs\n",
    "# Softmax Activation Function definition in README.md file\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Now we gotta compile the model into binary for computer to actually understand and execute it\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Now we need to train the model\n",
    "# epochs is how many times is the model going to see the same data all over again, standard 10-20 epochs\n",
    "# 10 epochs gives 99.5% accuracy and beyond that starts to slightly overfit due to diminishing returns\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "# After training the model we need to save it\n",
    "model.save(\"./models/handwritten.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>Loading the model, testing and prediction:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 603ms/step\n",
      "Digit 1 is probably a 3\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "Digit 2 is probably a 3\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Digit 3 is probably a 2\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Digit 4 is probably a 9\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Digit 5 is probably a 8\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Digit 6 is probably a 3\n"
     ]
    }
   ],
   "source": [
    "ready_model = tf.keras.models.load_model(\"../models/handwritten.model\")\n",
    "\n",
    "# loss, accuracy = ready_model.evaluate(x_test, y_test)\n",
    "\n",
    "# print(f\"Loss = {loss}\")  # 0.1% loss found\n",
    "# print(f\"Accuracy = {accuracy}\")  # 97% accuracy found\n",
    "\n",
    "image_number = 1\n",
    "\n",
    "while os.path.isfile(f\"../test/digit{image_number}.png\"):\n",
    "    try:\n",
    "        img = cv.imread(f\"../test/digit{image_number}.png\")[:, :, 0]\n",
    "        # inv_img = 1 - (img/255) # inverted image\n",
    "        # inv_img = np.array([inv_img], dtype=\"float32\")\n",
    "        # print(inv_img)\n",
    "        img = np.invert(np.array([img]))\n",
    "        # plt.imshow(img[0], cmap=\"gray\")\n",
    "        # plt.show()\n",
    "        # print(img/255)\n",
    "        img = img/255\n",
    "        prediction = ready_model.predict(img, batch_size=1)\n",
    "        print(f\"Digit {image_number} is probably a {np.argmax(prediction)}\")\n",
    "    except:\n",
    "        print(\"Some error occured\")\n",
    "    finally:\n",
    "        image_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the current working directory\n",
    "print(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
